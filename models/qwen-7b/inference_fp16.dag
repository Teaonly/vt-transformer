1e-06                   "RMS_EPS"               !
10000.0                 "ROTARY_BASE"           !
2.5                     "TEMPERATURE"           !

151936                  "VOCAB_SIZE"            !
4096                    "HIDDEN_SIZE"           !
11008                   "INTERMEDIATE_SIZE"     !
32                      "HEADS_NUM"             !
128                     "HEAD_HIDDEN"           !

2                       "MAX_BATCH"             !
1526                    "MAX_CONTEXT"           !
"./weights/"            "G_PATH"                !

%def create_input_weight
    $DEVICE !

    "VOCAB_SIZE" @ "HIDDEN_SIZE" @ 2 $DEVICE @ "fp16" op.create "wte.weight"  !

    $DEVICE !!
%end

%def create_output_weight
    $DEVICE !

    1 1 "HIDDEN_SIZE" @ 3 $DEVICE @ "fp16"  op.create  "ln_f.weight"  !
    "VOCAB_SIZE" @ "HIDDEN_SIZE" @ 2 $DEVICE @ "fp16" op.create "lm_head.weight" !

    $DEVICE !!
%end

%def create_layer_weight
    $L !
    $DEVICE !

    (1 1 "HIDDEN_SIZE" @  3 $DEVICE @ "fp16")  op.create  $L @ "ln_1.weight" | !
    (1 1 "HIDDEN_SIZE" @  3 $DEVICE @ "fp16")  op.create  $L @ "ln_2.weight"  | !

    ("HIDDEN_SIZE" @ 3 * "HIDDEN_SIZE" @ 2 $DEVICE @  "fp16" )  op.create  $L @  "attn.qkv_proj.weight" | !
    ("HIDDEN_SIZE" @ 3 *                 1 $DEVICE @  "fp16" )  op.create  $L @  "attn.qkv_proj.bias" | !
    ;
    ; dummy split to three tensors, 4096 * 4096 = 16777216
    ;
    ($L @ "attn.qkv_proj.weight" | @ 0                             "HIDDEN_SIZE" @ dup 2 op.view)  $L @  "query.weight" | !
    ($L @ "attn.qkv_proj.weight" | @ ("HIDDEN_SIZE" @ dup *)       "HIDDEN_SIZE" @ dup 2 op.view)  $L @  "key.weight" | !
    ($L @ "attn.qkv_proj.weight" | @ ("HIDDEN_SIZE" @ dup * 2 *)   "HIDDEN_SIZE" @ dup 2 op.view)  $L @  "value.weight" | !
    
    ($L @ "attn.qkv_proj.bias" | @ 0                               "HIDDEN_SIZE" @ 1 op.view)  $L @  "query.bias" | !
    ($L @ "attn.qkv_proj.bias" | @ ("HIDDEN_SIZE" @ )              "HIDDEN_SIZE" @ 1 op.view)  $L @  "key.bias" | !
    ($L @ "attn.qkv_proj.bias" | @ ("HIDDEN_SIZE" @ 2 *)           "HIDDEN_SIZE" @ 1 op.view)  $L @  "value.bias" | !
    
    ("HIDDEN_SIZE" @  "HIDDEN_SIZE" @ 2 $DEVICE @  "fp16")  op.create   $L @ "attn.c_proj.weight" |  !

    ("INTERMEDIATE_SIZE" @  "HIDDEN_SIZE" @ 2 $DEVICE @  "fp16")  op.create  $L @  "mlp.w1.weight"  | !
    ("INTERMEDIATE_SIZE" @  "HIDDEN_SIZE" @ 2 $DEVICE @  "fp16")  op.create  $L @  "mlp.w2.weight"  | !
    ("HIDDEN_SIZE" @  "INTERMEDIATE_SIZE" @ 2 $DEVICE @  "fp16")  op.create  $L @  "mlp.c_proj.weight" | !

    $L !!
    $DEVICE !!
%end

%def load_input_weight
    "Loading input weight..." ? 
    
    $weights_path ! 

    "wte.weight" @
    $weights_path @ "wte.fp16" |
    io.load

    $weights_path !!

    "Loaded input weight." ?
%end

%def load_output_weight
    "Loading output weight..." ? 
    
    $weights_path ! 
    
    "ln_f.weight" @
    $weights_path @ "ln_f.fp16"  |
    io.load

    "lm_head.weight" @
    $weights_path @ "lm_head.fp16"  |
    io.load

    $weights_path !!

    "Loaded output weight." ?
%end

%def load_layer_weight
    $L !
    $weights_path ! 
    
    "Loading... " $weights_path @ | ?
    
    $L @ "ln_1.weight"                     | @ $weights_path @  "ln_1.weight.fp16"                | io.load
    $L @ "ln_2.weight"                     | @ $weights_path @  "ln_2.weight.fp16"                | io.load
    $L @ "attn.qkv_proj.weight"            | @ $weights_path @  "attn.c_attn.weight.fp16"         | io.load
    $L @ "attn.qkv_proj.bias"              | @ $weights_path @  "attn.c_attn.bias.fp16"           | io.load
    $L @ "attn.c_proj.weight"              | @ $weights_path @  "attn.c_proj.weight.fp16"         | io.load
    $L @ "mlp.w1.weight"                   | @ $weights_path @  "mlp.w1.weight.fp16"              | io.load
    $L @ "mlp.w2.weight"                   | @ $weights_path @  "mlp.w2.weight.fp16"              | io.load
    $L @ "mlp.c_proj.weight"               | @ $weights_path @  "mlp.c_proj.weight.fp16"          | io.load
    
    "Loaded " $weights_path @ | ?
    
    $L !!
    $weights_path !!
%end

%def sync_layer_clone
    $L !

    $L @ "ln_1.weight"                        | @ "ln_1.weight"                       !
    $L @ "ln_2.weight"                        | @ "ln_2.weight"                       !
    $L @ "attn.qkv_proj.weight"               | @ "attn.qkv_proj.weight"              !
    $L @ "attn.qkv_proj.bias"                 | @ "attn.qkv_proj.bias"                !
    $L @ "attn.c_proj.weight"                 | @ "attn.c_proj.weight"                !
    $L @ "mlp.w1.weight"                      | @ "mlp.w1.weight"                     !
    $L @ "mlp.w2.weight"                      | @ "mlp.w2.weight"                     !
    $L @ "mlp.c_proj.weight"                  | @ "mlp.c_proj.weight"                 !

    ;
    ; dummy split to three tensors, 4096 * 4096 = 16777216
    ;
    ("attn.qkv_proj.weight" @ 0                             "HIDDEN_SIZE" @ dup 2 op.view)  "query.weight"  !
    ("attn.qkv_proj.weight" @ ("HIDDEN_SIZE" @ dup *)       "HIDDEN_SIZE" @ dup 2 op.view)  "key.weight"    !
    ("attn.qkv_proj.weight" @ ("HIDDEN_SIZE" @ dup * 2 *)   "HIDDEN_SIZE" @ dup 2 op.view)  "value.weight"  !
   
    ("attn.qkv_proj.bias" @ 0                             "HIDDEN_SIZE" @ 1 op.view)  "query.bias"  !
    ("attn.qkv_proj.bias" @ ("HIDDEN_SIZE" @ )            "HIDDEN_SIZE" @ 1 op.view)  "key.bias"    !
    ("attn.qkv_proj.bias" @ ("HIDDEN_SIZE" @ 2 *)         "HIDDEN_SIZE" @ 1 op.view)  "value.bias"  !

    $L !!
%end

%def init_internal_variable
    $DEVICE !

    ;; activity memory
    0.20 1024 1024 1024 * * * 1 $DEVICE @  "fp16" op.create  "_var_"  !
    
    ;; kv cached memroy
    32 2 1600 "HIDDEN_SIZE" @ 4 $DEVICE @  "fp16" op.create dup "_kcache_"  !
    32 2 1600 "HIDDEN_SIZE" @ 4 $DEVICE @  "fp16" op.create dup "_vcache_"  !
    nn.ezkv_init "cache_man" !

    "MAX_CONTEXT" @ "MAX_BATCH" @ * dup dup dup 
    1 "host" "int"  op.create  "_ids_"     !
    1 "host" "int"  op.create  "_mask_"    !
    1 $DEVICE @  "int"  op.create  "_ids"      !
    1 $DEVICE @  "int"  op.create  "_mask"     !
    1024 1 $DEVICE @  "int" op.create "_position" !
    
    10000 "HEAD_HIDDEN" @ 2 3 $DEVICE @  "float" op.create dup 
    "ROTARY_BASE" @ op.rotary_cache 
    "rotary_cache" !

    $DEVICE !!
%end

%def create_dynamic
    $batch          !
    $full_tokens    !
    $tokens         !
    
    ;; xinput in GPU
    {
        "_var_" @ 0  $batch @  $tokens @  "HIDDEN_SIZE" @ 3 op.view  "xinput" !
        $batch @ $tokens @ "HIDDEN_SIZE" @ * *
    }

    ;; causal mask, norm2 in GPU, extend to aligen address
    {
        dup
        "_var_" @ swap $batch @ 1 $tokens @  $full_tokens @ 4 "float" op.view_as  "causal_mask"  !
        $batch @ $tokens @ $full_tokens @ 2 * * * +

        dup
        "_var_" @ swap $batch @ $tokens @ 1 3 op.view "norm2" !
        $batch @ $tokens @ 2 * * +                                  
    }

    ;; xa, xb, xquery/key/value, xll, x4a, x4b, all_logits
    dup
    "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xa" !
    $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
    "xa" @ 0 $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "ya" !
    "xa" @ 0 $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "za" !

    dup
    "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xb" !
    $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
    "xb" @ 0 $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "yb" !
    "xb" @ 0 $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "zb" !
    
    dup 
    {
        dup
        "_var_" @ swap $batch @ $tokens @ "HIDDEN_SIZE" @ 3 op.view "xc" !
        $batch @ $tokens @ "HIDDEN_SIZE" @  * * +
        "xc" @ 0 $batch @ $tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "yc" !
        "xc" @ 0 $batch @ "HEADS_NUM" @ $tokens @ "HEAD_HIDDEN" @ 4 op.view "zc" !
      
        dup
        "_var_" @ swap $batch @ $full_tokens @ "HIDDEN_SIZE" @ 3 op.view "xfa" !
        $batch @ $full_tokens @ "HIDDEN_SIZE" @  * * +
        "xfa" @ 0 $batch @ $full_tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "yfa" !
        "xfa" @ 0 $batch @ "HEADS_NUM" @ $full_tokens @ "HEAD_HIDDEN" @ 4 op.view "zfa" !

        dup
        "_var_" @ swap $batch @ $full_tokens @ "HIDDEN_SIZE" @ 3 op.view "xfb" !
        $batch @ $full_tokens @ "HIDDEN_SIZE" @  * * +
        "xfb" @ 0 $batch @ $full_tokens @ "HEADS_NUM" @ "HEAD_HIDDEN" @ 4 op.view "yfb" !
        "xfb" @ 0 $batch @ "HEADS_NUM" @ $full_tokens @ "HEAD_HIDDEN" @ 4 op.view "zfb" !

        dup
        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ $full_tokens @ 4 op.view "xll_half" !
        $batch @ "HEADS_NUM" @ $tokens @ $full_tokens @ * * * +

        "_var_" @ swap $batch @ "HEADS_NUM" @ $tokens @ $full_tokens @ 4 "float" op.view_as "xll" !
    }  
    dup
    {
   
        dup 
        "_var_" @ swap $batch @ $tokens @ "INTERMEDIATE_SIZE" @ 3 op.view "x4a" !
        $batch @ $tokens @ "INTERMEDIATE_SIZE" @ * * +

        "_var_" @ swap $batch @ $tokens @ "INTERMEDIATE_SIZE" @ 3 op.view "x4b" !
    }
    "_var_" @ swap 0 "VOCAB_SIZE" @ 2 op.view  "all_logits" !

    $tokens !!
    $batch !!
    $full_tokens !!
%end

%def prepare_input
    {
        $tokens !
        $batch !
        
        "_ids_"  @ 0 $batch @ $tokens @ 2 op.view  "ids_" !
        "_mask_" @ 0 $batch @ $tokens @ 2 op.view  "mask_" !
        
        "ids_" @  io.pipe.read
        "mask_" @ io.pipe.read

        $batch !!
        $tokens !!
    }
   
    ;; fetch KV cache
    "cache_man" @  "ids_" @  "mask_" @  "_ids" @ "_mask" @ nn.ezkv_match  
    "mask" !        
    "ids"  !
       
    ;; write cached mask & ids back to cpu
    "_mask_" @ 0 "mask" @ op.get_shape op.view "mask_" !
    "_ids_" @ 0 "ids" @ op.get_shape op.view "ids_" !
    
    "mask_" @ "mask" @ op.copy
    "ids_" @ "ids" @ op.copy

    {
        "ids" @ op.get_shape drop swap drop
        "mask" @ op.get_shape drop swap 
        
        create_dynamic
    }
    "mask" @ "causal_mask" @ op.causal_mask
%end



%def gpu_init
    "dcu"       init_internal_variable
 
    "host"      create_input_weight
    "dcu"       create_output_weight
    
    "dcu"    "L0."   create_layer_weight 
    "dcu"    "L1."   create_layer_weight 
    "dcu"    "L2."   create_layer_weight 
    "dcu"    "L3."   create_layer_weight 
    "dcu"    "L4."   create_layer_weight
    "dcu"    "L5."   create_layer_weight 
    "dcu"    "L6."   create_layer_weight 
    "dcu"    "L7."   create_layer_weight 
    "dcu"    "L8."   create_layer_weight 
    "dcu"    "L9."   create_layer_weight
    "dcu"    "L10."  create_layer_weight
    "dcu"    "L11."  create_layer_weight
    "dcu"    "L12."  create_layer_weight
    "dcu"    "L13."  create_layer_weight
    "dcu"    "L14."  create_layer_weight
    "dcu"    "L15."  create_layer_weight
    "dcu"    "L16."   create_layer_weight 
    "dcu"    "L17."   create_layer_weight 
    "dcu"    "L18."   create_layer_weight 
    "dcu"    "L19."   create_layer_weight 
    "dcu"    "L20."   create_layer_weight
    "dcu"    "L21."   create_layer_weight 
    "dcu"    "L22."   create_layer_weight 
    "dcu"    "L23."   create_layer_weight 
    "dcu"    "L24."   create_layer_weight 
    "dcu"    "L25."   create_layer_weight
    "dcu"    "L26."   create_layer_weight
    "dcu"    "L27."   create_layer_weight
    "dcu"    "L28."   create_layer_weight
    "dcu"    "L29."   create_layer_weight
    "dcu"    "L30."   create_layer_weight
    "dcu"    "L31."   create_layer_weight
    
    "G_PATH" @ load_input_weight
    "G_PATH" @ load_output_weight

    "G_PATH" @ "h_0."  | "L0." load_layer_weight
    "G_PATH" @ "h_1."  | "L1." load_layer_weight
    "G_PATH" @ "h_2."  | "L2." load_layer_weight
    "G_PATH" @ "h_3."  | "L3." load_layer_weight
    "G_PATH" @ "h_4."  | "L4." load_layer_weight
    "G_PATH" @ "h_5."  | "L5." load_layer_weight
    "G_PATH" @ "h_6."  | "L6." load_layer_weight
    "G_PATH" @ "h_7."  | "L7." load_layer_weight
    "G_PATH" @ "h_8."  | "L8." load_layer_weight
    "G_PATH" @ "h_9."  | "L9." load_layer_weight
    "G_PATH" @ "h_10."  | "L10." load_layer_weight
    "G_PATH" @ "h_11."  | "L11." load_layer_weight
    "G_PATH" @ "h_12."  | "L12." load_layer_weight
    "G_PATH" @ "h_13."  | "L13." load_layer_weight
    "G_PATH" @ "h_14."  | "L14." load_layer_weight
    "G_PATH" @ "h_15."  | "L15." load_layer_weight
    "G_PATH" @ "h_16."  | "L16." load_layer_weight
    "G_PATH" @ "h_17."  | "L17." load_layer_weight
    "G_PATH" @ "h_18."  | "L18." load_layer_weight
    "G_PATH" @ "h_19."  | "L19." load_layer_weight
    "G_PATH" @ "h_20."  | "L20." load_layer_weight
    "G_PATH" @ "h_21."  | "L21." load_layer_weight
    "G_PATH" @ "h_22."  | "L22." load_layer_weight
    "G_PATH" @ "h_23."  | "L23." load_layer_weight
    "G_PATH" @ "h_24."  | "L24." load_layer_weight
    "G_PATH" @ "h_25."  | "L25." load_layer_weight
    "G_PATH" @ "h_26."  | "L26." load_layer_weight
    "G_PATH" @ "h_27."  | "L27." load_layer_weight
    "G_PATH" @ "h_28."  | "L28." load_layer_weight
    "G_PATH" @ "h_29."  | "L29." load_layer_weight
    "G_PATH" @ "h_30."  | "L30." load_layer_weight
    "G_PATH" @ "h_31."  | "L31." load_layer_weight
%end

%def gpu_main
    prepare_input
 
    ;; embed    
    {
        "ids" @ "wte.weight" @ "xinput" @ op.embed
    }

%end

